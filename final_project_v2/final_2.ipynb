{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Финальный проект. Вариант 2. Шутков Д.Н.\n",
        "\n",
        "1. Парсинг биржевых данных о движении цен на акции Meta, Netflix, Apple, Amazon, Google.\n",
        "\n",
        "2. Очистка, преобразование данных.\n",
        "\n",
        "3. Построение на основании этих данных витрин: \n",
        "Название акции;\n",
        "Суммарный объем торгов за последние сутки; Цена акции на момент открытия торгов для данных суток; Цена акции на момент закрытия торгов для данных суток;\n",
        "Разница(в %)цен с момента открытия до момента закрытия торгов для данных суток; Минимальный временной интервал, на котором был зафиксирован самый крупный объем торгов для данных суток; Минимальный временной интервал, на котором была зафиксирована максимальная цена для данных суток; Минимальный временной интервал, на котором была зафиксирована минимальная цена торгов для данных суток\n",
        "\n"
      ],
      "metadata": {
        "id": "7HiN9VE37ntN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "├── data                       # Архив с файлами исходных данных <br/>\n",
        "├── data marts                 # Витрины данных в формате parquet<br/>\n",
        "├── docs                       # Презентация<br/>\n",
        "├── idef                       # Функциональная модель проекта <br/>\n",
        "├── src                        # Исходный код программы <br/>\n",
        "└── README.md                  # Информация о проекте<br/>\n"
      ],
      "metadata": {
        "id": "U97FrbUX8lmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. ЗАГРУЗКА ДАННЫХ "
      ],
      "metadata": {
        "id": "Ch7_v2xC_cZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка данных с API сайта https://www.alphavantage.co\n",
        "Данные загружаются в папку data в виде csv файлов,\n",
        "именованных по формату \"ГГГГ_ММ_ДД_ТИКЕТАКЦИИ\""
      ],
      "metadata": {
        "id": "A-IjkRTdn3Yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Параметры для скачивания с API\n",
        "\"\"\"API Parameters\n",
        "❚ Required: function\n",
        "The time series of your choice. In this case, function=TIME_SERIES_INTRADAY\n",
        "❚ Required: symbol\n",
        "The name of the equity of your choice. For example: symbol=IBM\n",
        "❚ Required: interval\n",
        "Time interval between two consecutive data points in the time series. The following values are supported: 1min, 5min, 15min, 30min, 60min\n",
        "❚ Optional: adjusted\n",
        "By default, adjusted=true and the output time series is adjusted by historical split and dividend events. Set adjusted=false to query raw (as-traded) intraday values.\n",
        "❚ Optional: outputsize\n",
        "By default, outputsize=compact. Strings compact and full are accepted with the following specifications: compact returns only the latest 100 data points in the intraday time series; full returns the full-length intraday time series. The \"compact\" option is recommended if you would like to reduce the data size of each API call.\n",
        "❚ Optional: datatype\n",
        "By default, datatype=json. Strings json and csv are accepted with the following specifications: json returns the intraday time series in JSON format; csv returns the time series as a CSV (comma separated value) file.\n",
        "❚ Required: apikey\n",
        "Your API key. Claim your free API key here. \"\"\""
      ],
      "metadata": {
        "id": "htSRkad13VIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Импорт необходимых библиотек\n",
        "import csv\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "from os import path\n",
        "import dateutil\n",
        "import datetime\n",
        "import shutil\n",
        "\n",
        "#ПЕРЕНЕСТИ В ОТДЕЛЬНЫЙ ФАЙЛ ТАК КАК ИСПОЛЬЗУЮТСЯ ЧАСТО\n",
        "today = datetime.date.today()\n",
        "previous_date = today - dateutil.relativedelta.relativedelta(days=1)\n",
        "year = previous_date.year\n",
        "month = previous_date.month\n",
        "day = previous_date.day\n",
        "\n",
        "#Секретный ключ APIKEY берется из отдельного файла с целью защиты информации от несанцкионированного доступа\n",
        "with open('/content/api-key.txt',mode=\"r\") as file:\n",
        "    api_key = file.readline().strip()\n",
        "\n",
        "#список акций с которыми будем работать\n",
        "stocks_list = ['META','AAPL', 'AMZN', 'NFLX', 'GOOG']\n",
        "\n",
        "#Проверим что директория для хранения \"сырых\" данных существует, если нет - создадим\n",
        "if path.exists('/content/data') == False:\n",
        "  os.mkdir('/content/data')\n",
        "\n",
        "#Циклом проходим по списку акций и загружаем \"сырые\" данные по API\n",
        "#Данные по каждой акции ложим в папку Data, имена файлов в формате \"ГГГГ_ММ_ДД_ТИКЕТАКЦИИ\"\n",
        "for stock_name in stocks_list:\n",
        "  url_for_downloading = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&'\\\n",
        "  f'symbol={stock_name}&interval=1min&slice=year1month1&apikey={api_key}'\n",
        "  with requests.Session() as s:\n",
        "    download = s.get(url_for_downloading)\n",
        "    decoded_content = download.content.decode('utf-8')\n",
        "    data_from_csv = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
        "    temp_data_list = list(data_from_csv)\n",
        "\n",
        "#Пока с костылями в виде промежуточных df, далее ИСПРАВИТЬ \n",
        "    df=pd.DataFrame(temp_data_list,columns=['time','open','high','low','close','volume'])\n",
        "    df.drop (index=df.index [0], axis= 0 , inplace= True )\n",
        "    path_for_download_data = '/content/data/'\n",
        "    file_name = f'{year}_{month}_{day}_{stock_name}.csv'\n",
        "\n",
        "#Если файлы уже существуют, то удаляем их: вероятно они повреждены, если мы загружаем их снова\n",
        "    if path.exists(f'{path_for_download_data}{file_name}') == True:\n",
        "      os.remove(f'{path_for_download_data}{file_name}')\n",
        "    df.to_csv(f'{path_for_download_data}{file_name}')\n",
        "    df.head()#ВРЕМЕННО ДЛЯ ПРОСМОТРА НА СТАДИИ РАЗРАБОТКИ"
      ],
      "metadata": {
        "id": "9kKzyE6q_hUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UYlMIV5Gl3tK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Процесс Data quality"
      ],
      "metadata": {
        "id": "4kEGPQwOl4bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скопируем данные, которые будем обрабатывать, в промежуточный слой\n",
        "в нем произведем очистку данных и необходимые преобразования\n",
        "и выложим в слой витрин готовые данные"
      ],
      "metadata": {
        "id": "yq6JOQl6Cs7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_for_copy_data = '/content/data_transform/'\n",
        "#Проверим что директория для хранения промежуточных данных существует, если нет - создадим\n",
        "if path.exists(path_for_copy_data) == False:\n",
        "  os.mkdir(path_for_copy_data)\n",
        "\n",
        "#Циклом копируем файлы в слой их обработки\n",
        "for stock_name in stocks_list:\n",
        "  file_name = f'{year}_{month}_{day}_{stock_name}.csv'\n",
        "\n",
        "#Если файлы уже существуют, то удаляем их: вероятно они повреждены, если мы копируем их снова\n",
        "  if path.exists(f'{path_for_copy_data}{file_name}') == True:\n",
        "    os.remove(f'{path_for_copy_data}{file_name}')\n",
        "  shutil.copy(f'{path_for_download_data}{file_name}',path_for_copy_data)\n",
        "\n",
        "df=pd.read_csv(f'{path_for_copy_data}{file_name}')\n",
        "df.head(20)\n"
      ],
      "metadata": {
        "id": "biRIFdb5lBNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Построение витрин"
      ],
      "metadata": {
        "id": "F8LnNg7z0NG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    ВЫХОДНЫЕ ДАННЫЕ:\n",
        "    Суррогатный ключ категории\n",
        "    Название акции\n",
        "    Суммарный объем торгов за последние сутки\n",
        "    Цена акции на момент открытия торгов для данных суток\n",
        "    Цена акции на момент закрытия торгов для данных суток\n",
        "    Разница(в %) цен с момента открытия до момента закрытия торгов для данных суток\n",
        "    Минимальный временной интервал, на котором был зафиксирован самый крупный объем торгов для данных суток\n",
        "    Минимальный временной интервал, на котором была зафиксирована максимальная цена для данных суток\n",
        "    Минимальный временной интервал, на котором была зафиксирована минимальная цена торгов для данных суток"
      ],
      "metadata": {
        "id": "o2YezNHF0Tvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Предполагается выгрузка данных в Postgres\n",
        "#далее выгрузка из него витрин в отдельный слой (папка data_marts)"
      ],
      "metadata": {
        "id": "gdpJW4jx0WP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ЗАГОТОВКИ ДЛЯ РАБОТЫ С AIRFLOW\n",
        "# !pip install apache-airflow\n",
        "# !airflow initdb\n",
        "# !airflow webserver -p 8080"
      ],
      "metadata": {
        "id": "BDnKY0dH5riC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ЗАГОТОВКИ ДЛЯ РАБОТЫ С AIRFLOW\n",
        "# from datetime import datetime\n",
        "# from airflow import DAG\n",
        "# from airflow.operators.bash import BashOperator\n",
        "# from airflow.operators.python import PythonOperator\n",
        "# from airflow.models import Variable\n",
        "# import pandas as pd\n",
        "# import json\n",
        "\n",
        "# dag_downloading_args = {\n",
        "# \t'owner': 'Dmitrii',\n",
        "# \t'depends_on_past': False,\n",
        "# \t'start_date': datetime(2022, 12, 30, tz='Europe/Moscow'),\n",
        "# \t'email': ['shutkov.dmitry2013@yandex.ru'],\n",
        "# \t'email_on_failure': True,\n",
        "# \t'email_on_retry': True,\n",
        "# \t'retries': 3,\n",
        "# \t'retry_delay': timedelta(minutes=5),\n",
        "# \t'schedule_interval': '@dayly',\n",
        "# }  \n",
        "\n",
        "# dag = DAG(\n",
        "#     dag_id='stocks_information_dayly',\n",
        "#     schedule_interval='30 1 * * *',\n",
        "#     catchup=False,\n",
        "#     default_args=dag_downloading_args\n",
        "# )\n",
        "\n",
        "\n",
        "# # a.Скачайте произвольный csv-файл (ссылка на файл указана в Variables)\n",
        "# def read_csv():\n",
        "#     url = Variable.get(\"url_csv\")\n",
        "#     r = pd.read_csv(url)\n",
        "#     return r.to_json()\n",
        "\n",
        "# # В этой функции производится подсчёт строк\n",
        "# def count_r(**kwargs):\n",
        "#     json_table = kwargs['ti']\n",
        "#     r = json_table.xcom_pull(task_ids='read')\n",
        "#     r = json.loads(r)\n",
        "#     return len(r[\"Game Number\"])\n",
        "\n",
        "# #  b. Создайте еще один python-оператор, передайте в него из первого python-оператора количество строк. \n",
        "# # Прочитайте файл(с использованием переменных) и добавьте к данному файлу колонку справа, \n",
        "# # которая будет номеровать строки в обратном порядке \n",
        "# # Сохраните полученный файл.\n",
        "# def create_column(**kwargs):\n",
        "#     # Вытаскиваем результаты из предыдущих операторов с помощью Xcom\n",
        "#     input = kwargs[\"ti\"]\n",
        "#     table = input.xcom_pull(task_ids = \"read\")\n",
        "#     count = input.xcom_pull(task_ids = \"count\")\n",
        "\n",
        "#     df = pd.read_json(table)\n",
        "#     reverse_list = []\n",
        "#     for i in range(count, 0, -1):\n",
        "#         reverse_list.append(i)\n",
        "   \n",
        "#     # Добавляем столбец в DataFrame, конвертируем в csv и сохраняем.\n",
        "#     df[\"reverse\"] = reverse_list\n",
        "#     df.to_csv('files/file.csv')\n",
        "\n",
        "# with DAG(dag_id=\"a_start_dag\", start_date=datetime(2022, 1, 1), schedule=\"0 0 * * *\") as dag:\n",
        "\n",
        "#     python_read = PythonOperator(task_id = \"read\", python_callable = read_csv)\n",
        "#     python_count = PythonOperator(task_id = \"count\", python_callable = count_r)\n",
        "#     python_create_column = PythonOperator(task_id = \"create\", python_callable = create_column)\n",
        "\n",
        "#     bash_read = BashOperator(task_id = \"bash_cat\", bash_command = \"mv /opt/airflow/files/file.csv /opt/airflow/finish/\")\n",
        "#     bash_success = BashOperator(task_id = \"Success\", bash_command = \"echo Success\")\n",
        "\n",
        "#     python_read >> python_count >> python_create_column >> bash_read >> bash_success\n"
      ],
      "metadata": {
        "id": "B-OYWm6L5XGm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}