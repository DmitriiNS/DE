# Финальный проект по курсу DataEngeneer. Вариант 2. 
Анализ рынка акций

# Описание проекта

Имеется сайт https://www.alphavantage.co/, который позволяет по API парсить данные по тикету 5-10 акций с интервалом от 1 мин до 60 мин. Данные доступны за период до двух лет. 

# Задача:

Разработать скрипты загрузки данных в 2-х режимах:

o Инициализирующий – загрузка полного слепка данных источника

o Инкрементальный – загрузка дельты данных за прошедшие сутки

Организовать правильную структуру хранения данных

o Сырой слой данных

o Промежуточный слой

В качестве результата работы программного продукта необходимо написать скрипт, который формирует витрину данных следующего содержания:

Суррогатный ключ категории
Название акции
Суммарный объем торгов за последние сутки
Цена акции на момент открытия торгов для данных суток
Цена акции на момент закрытия торгов для данных суток
Разница(в %) цены с момента открытия до момента закрытия торгов для данных суток
Минимальный временной интервал, на котором был зафиксирован самый крупный объем торгов для данных суток
Минимальный временной интервал, на котором была зафиксирована максимальная цена для данных суток
Минимальный временной интервал, на котором была зафиксирована минимальная цена торгов для данных суток

# Используемые технологии с обоснованием

Данный проект был выполнен локально, со следующим системным окружением:
OS: Windows 11 
Docker Desktop 4.15.0
Apache Airflow:2.4.3
Python: 3.8
Postgres: 15 версия

Средство разработки jupyter notebook + IntelliJ IDEA 2022.1.2

Поскольку данный проект предполагает обработку данных по расписанию и данные постоянно пополняются, то в качестве решения была предложена локальная схема с Docker и Airflow. По схеме: загрузка-> обработка и преобразование ->выгрузка.
В качестве технологического стека выбран Python. Airflow создан именно на этом языке и позволяет полноценно его использовать в Dags. Python имеет библиотеку Pandas для работы с данными,которая является основным инструментом распределенной обработки данных, а также технологическим решением для преобразования данных в самых различных задачах DE. К тому же, Python поддерживает самые различные форматы данных.

# Схема/архитектура

Архитектура проекта: исходные данные загружаются в DateFrame и выгружаются в csv файлы в локальную папку /data. Далее информация из них копируется в DataFrame и после некоторых трансформаций ложится в базу Postgres. Потом результат запроса к Postgres выгружается в DataFrame и встроенными средствами Pandas кладет parquet файлы в локальную папку /data marts. 

![Screenshot](https://github.com/DmitriiNS/DE/blob/develop/final_project_v2/images/1_%D0%A1%D1%85%D0%B5%D0%BC%D0%B0%20%D0%BF%D1%80%D0%BE%D0%B5%D0%BA%D1%82%D0%B0.jpg)

# План реализации

# 1. Подготовка системного окружения 

Подготовка системного окружения включает в себя установку и настройку следующих инструментов, программ и СУБД:
* OS: Windows 11 
* Docker Desktop 4.15.0
* Apache Airflow:2.4.3
* Python: 3.8
* Postgres: 15 версия
* Средстство разработки jupyter notebook + IntelliJ IDEA 2022.1.2

Docker desctop был установлен локально. СУБД postgres был установлен локально через Docker. Был использован официальный образ с postgres и airflow сайта DockerHUB.

# 2. Загрузка данных

* 2.1 Инициализирующий режим – загрузка полного слепка данных источника (согласно возможностей источника это 2 года. 
Скачивание по API происходит в файлы csv. Данные файлы сохраняются как "сырой слой" именованные по формату "ГГГГ_ММ_ТИКЕТАКЦИИ" в папку /Data

![Screenshot](https://github.com/DmitriiNS/DE/blob/develop/final_project_v2/images/2_Dag_init.jpg)
* 2.2 Инкрементальный режим загрузки: загрузка данных за вчерашний день. Скачивание по API происходит в файлы csv. Данные файлы сохраняются как "сырой слой" именованные по формату "ГГГГ_ММ_ДД_ТИКЕТАКЦИИ" в папку /Data 

# 3. Преобразование данных и загрузка в Postgres 

Далее копируются данные для обработки в промежуточный слой: ДатаФреймы Пандас. В ДатаФреймах производится очистка данных и необходимые преобразования: разбиение колонки time на отдельные date и time, далее time преобразуется в time_interval. Обработанные данные загружаются в базу Postgres

![Screenshot](https://github.com/DmitriiNS/DE/blob/develop/final_project_v2/images/3_Dag_ETL.jpg)

# 4. Построение витрин.

В базе Postgres с помощью языка запросов формируются таблицы согласно ТЗ, передаются в пандас ДатаФрейм, преобразуются в файлы в формате parquet и сохраняются в папку /data marts согласно формата "datamart_{stock_name}_{date}"

Пример витрины datamart_AAPL_2023-01-06

![Screenshot](https://github.com/DmitriiNS/DE/blob/develop/final_project_v2/images/4_%D0%9F%D1%80%D0%B8%D0%BC%D0%B5%D1%80%20%D0%B2%D0%B8%D1%82%D1%80%D0%B8%D0%BD%D1%8B.jpg)


# Результаты

Результаты данной реализации и проектирования представлены в следующую структуру:

![Screenshot](https://github.com/DmitriiNS/DE/blob/develop/final_project_v2/images/5_%D1%80%D0%B5%D0%B7%D1%83%D0%BB%D1%8C%D1%82%D0%B0%D1%82%20%D0%BF%D1%80%D0%BE%D0%B5%D0%BA%D1%82%D0%B0.jpg)

